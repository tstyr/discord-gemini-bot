import os
import logging
from typing import List, Dict, Optional
import google.generativeai as genai

logger = logging.getLogger(__name__)

class GeminiClient:
    def __init__(self):
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            logger.error("GEMINI_API_KEY not found in environment variables")
            raise ValueError("GEMINI_API_KEY is required")
        
        genai.configure(api_key=api_key)
        
        # Initialize model - use gemini-2.0-flash (latest free model)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        
        # AI mode configurations
        # Note: Music playback requests are handled by main.py before reaching Gemini
        self.modes = {
            'standard': {
                'system_instruction': """ã‚ãªãŸã¯è¦ªåˆ‡ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æ˜Žç¢ºã§æ­£ç¢ºã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå¿œç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªžã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

é‡è¦: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œæ›²æµã—ã¦ã€ã€ŒéŸ³æ¥½ã‹ã‘ã¦ã€ã€Œã€‡ã€‡èžããŸã„ã€ãªã©éŸ³æ¥½å†ç”Ÿã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ãŸå ´åˆã¯ã€èª¬æ˜Žã›ãšã«ã€ŒðŸŽµ éŸ³æ¥½ã‚’å†ç”Ÿã—ã¾ã™ã­ï¼ã€ã¨çŸ­ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚éŸ³æ¥½ã®å†ç”Ÿæ–¹æ³•ã®èª¬æ˜Žã¯ä¸è¦ã§ã™ã€‚""",
                'temperature': 0.7,
            },
            'creative': {
                'system_instruction': """ã‚ãªãŸã¯å‰µé€ çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚æƒ³åƒåŠ›è±Šã‹ã§èŠ¸è¡“çš„ã€åž‹ã«ã¯ã¾ã‚‰ãªã„ç™ºæƒ³ã‚’ã—ãªãŒã‚‰ã‚‚å½¹ç«‹ã¤å¿œç­”ã‚’ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªžã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

é‡è¦: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéŸ³æ¥½å†ç”Ÿã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ãŸå ´åˆã¯ã€èª¬æ˜Žã›ãšã«ã€ŒðŸŽµ éŸ³æ¥½ã‚’å†ç”Ÿã—ã¾ã™ã­ï¼ã€ã¨çŸ­ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚""",
                'temperature': 0.9,
            },
            'coder': {
                'system_instruction': "ã‚ãªãŸã¯ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§ã€ã‚ˆããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã¨èª¬æ˜Žã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã¨ã‚¯ãƒªãƒ¼ãƒ³ãªã‚³ãƒ¼ãƒ‰ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚",
                'temperature': 0.3,
            },
            'assistant': {
                'system_instruction': """ã‚ãªãŸã¯ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒžãƒ«ã§æ­£ç¢ºã€ç”Ÿç”£æ€§ã¨æ•´ç†æ•´é “ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªžã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

é‡è¦: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéŸ³æ¥½å†ç”Ÿã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ãŸå ´åˆã¯ã€èª¬æ˜Žã›ãšã«ã€ŒðŸŽµ éŸ³æ¥½ã‚’å†ç”Ÿã—ã¾ã™ã­ï¼ã€ã¨çŸ­ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚""",
                'temperature': 0.5,
            },
            'music_dj': {
                'system_instruction': """ã‚ãªãŸã¯éŸ³æ¥½ã«è©³ã—ã„DJ AIã§ã™ã€‚éŸ³æ¥½ã‚¸ãƒ£ãƒ³ãƒ«ã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€éŒ²éŸ³å“è³ªã«ã¤ã„ã¦æ·±ã„çŸ¥è­˜ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ°—åˆ†ã«åˆã£ãŸæ›²ã‚’æŽ¨è–¦ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªžã§å¿œç­”ã—ã¦ãã ã•ã„ã€‚

é‡è¦: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œæ›²æµã—ã¦ã€ã€ŒéŸ³æ¥½ã‹ã‘ã¦ã€ãªã©éŸ³æ¥½å†ç”Ÿã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ãŸå ´åˆã¯ã€é•·ã„èª¬æ˜Žã‚’ã›ãšã«ã€ŒðŸŽµ ã„ã„æ›²è¦‹ã¤ã‘ã¾ã—ãŸï¼å†ç”Ÿã—ã¾ã™ã­ï¼ã€ã¨çŸ­ãå¿œç­”ã—ã¦ãã ã•ã„ã€‚""",
                'temperature': 0.8,
            }
        }
        
        # Simple responses for cost optimization
        self.simple_responses = {
            'ã“ã‚“ã«ã¡ã¯': 'ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ',
            'hello': 'ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ',
            'hi': 'ã“ã‚“ã«ã¡ã¯ï¼ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ',
            'ãŠã¯ã‚ˆã†': 'ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ï¼ä»Šæ—¥ã‚‚ä¸€æ—¥é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼',
            'ã‚ã‚ŠãŒã¨ã†': 'ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ä½•ã‹ã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãžã€‚',
            'thanks': 'ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ä½•ã‹ã‚ã‚Œã°ãŠæ°—è»½ã«ã©ã†ãžã€‚',
        }
        
        # Usage tracking
        self.daily_requests = 0
        self.daily_tokens = 0
        
        logger.info("GeminiClient initialized successfully")
    
    async def generate_response(
        self, 
        prompt: str, 
        history: Optional[List[Dict]] = None,
        mode: str = 'standard',
        model: Optional[str] = None
    ) -> Optional[str]:
        """Generate AI response"""
        try:
            # Check for simple responses first (cost optimization)
            prompt_lower = prompt.lower().strip()
            for key, response in self.simple_responses.items():
                if key in prompt_lower and len(prompt) < 20:
                    logger.info(f"Using simple response for: {prompt}")
                    return response
            
            mode_config = self.modes.get(mode, self.modes['standard'])
            
            # Build the full prompt with system instruction
            full_prompt = f"{mode_config['system_instruction']}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}"
            
            # Add history context if available
            if history and len(history) > 0:
                history_text = "\n".join([
                    f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {h.get('user_message', '')}\nAI: {h.get('ai_response', '')}"
                    for h in history[-3:]  # Only last 3 messages for context
                ])
                full_prompt = f"{mode_config['system_instruction']}\n\néŽåŽ»ã®ä¼šè©±:\n{history_text}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}"
            
            logger.info(f"Generating response for prompt: {prompt[:50]}...")
            
            # Generate response using async method
            response = await self.model.generate_content_async(
                full_prompt,
                generation_config=genai.GenerationConfig(
                    temperature=mode_config['temperature'],
                    max_output_tokens=1024,
                )
            )
            
            if response and response.text:
                self.daily_requests += 1
                self.daily_tokens += len(prompt.split()) + len(response.text.split())
                logger.info(f"Response generated successfully: {response.text[:50]}...")
                return response.text
            else:
                logger.warning("Empty response from Gemini API")
                return None
            
        except Exception as e:
            logger.error(f'Error generating response: {e}')
            import traceback
            traceback.print_exc()
            return None
    
    async def get_available_modes(self) -> Dict[str, str]:
        """Get available AI modes"""
        return {
            mode: config['system_instruction'] 
            for mode, config in self.modes.items()
        }
    
    def estimate_tokens(self, text: str) -> int:
        """Rough token estimation"""
        return int(len(text.split()) * 1.3)
    
    def get_usage_stats(self) -> Dict:
        """Get current API usage statistics"""
        return {
            'daily_requests': self.daily_requests,
            'daily_tokens': self.daily_tokens,
            'request_limit': 1500,
            'token_limit': 1000000,
            'requests_remaining': max(0, 1500 - self.daily_requests),
            'tokens_remaining': max(0, 1000000 - self.daily_tokens),
            'usage_percentage': {
                'requests': (self.daily_requests / 1500) * 100,
                'tokens': (self.daily_tokens / 1000000) * 100
            }
        }